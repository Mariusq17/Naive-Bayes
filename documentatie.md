# DocumentaÈ›ie Proiect: Clasificator Multinomial Naive Bayes
**Autor:** Ignat Marius Florentin  
**Data:** 16 Noiembrie 2025  
**Curs:** ProbabilitaÈ›i È™i statisticÄƒ  
**Universitatea:** Universitatea din BucureÈ™ti

---

## Cuprins
1. [Introducere](#1-introducere)
2. [Modelul Matematic](#2-modelul-matematic)
3. [Structura Codului](#3-structura-codului)
4. [InstrucÈ›iuni de Utilizare](#4-instrucÈ›iuni-de-utilizare)
5. [Exemple de Utilizare](#5-exemple-de-utilizare)
6. [Rezultate È™i Evaluare](#6-rezultate-È™i-evaluare)
7. [ReferinÈ›e Bibliografice](#7-referinÈ›e-bibliografice)

---

## 1. Introducere

### 1.1 Descrierea Problemei
Acest proiect implementeazÄƒ un **clasificator Multinomial Naive Bayes** pentru clasificarea automatÄƒ a articolelor de È™tiri BBC Ã®n categorii tematice. Clasificatorul a fost implementat de la zero, fÄƒrÄƒ utilizarea bibliotecilor de machine learning precum scikit-learn.

### 1.2 Dataset
- **SursÄƒ**: Articole de È™tiri BBC (42,115 articole)
- **Perioada**: 2013-2024
- **Atribute**: titlu, descriere, link, datÄƒ publicare
- **Categorii finale**: 6 categorii principale (business, entertainment, health-education, tech-science, uk, world)

### 1.3 Obiective
1. Procesarea È™i curÄƒÈ›area datelor text
2. Implementarea algoritmului Naive Bayes din ground-up
3. Evaluarea performanÈ›ei pe un set de test independent
4. Optimizarea categoriilor pentru maximizarea acurateÈ›ei

---

## 2. Modelul Matematic

### 2.1 Teorema lui Bayes

Clasificatorul Naive Bayes se bazeazÄƒ pe **Teorema lui Bayes**, care defineÈ™te probabilitatea condiÈ›ionatÄƒ:

```
P(C|D) = P(D|C) Ã— P(C) / P(D)
```

Unde:
- **P(C|D)** = probabilitatea ca un document D sÄƒ aparÈ›inÄƒ categoriei C (probabilitate *a posteriori*)
- **P(D|C)** = probabilitatea de a observa documentul D dacÄƒ aparÈ›ine categoriei C (*likelihood*)
- **P(C)** = probabilitatea *a priori* a categoriei C
- **P(D)** = probabilitatea documentului D (constantÄƒ pentru toate categoriile)

### 2.2 Ipoteza "NaivÄƒ"

Algoritmul presupune **independenÈ›a condiÈ›ionatÄƒ** Ã®ntre cuvinte, adicÄƒ:

```
P(D|C) = P(wâ‚, wâ‚‚, ..., wâ‚™|C) = P(wâ‚|C) Ã— P(wâ‚‚|C) Ã— ... Ã— P(wâ‚™|C)
```

AceastÄƒ ipotezÄƒ simplificÄƒ calculele, deÈ™i Ã®n realitate cuvintele sunt corelate. Cu toate acestea, Ã®n practicÄƒ, Naive Bayes funcÈ›ioneazÄƒ surprinzÄƒtor de bine.

### 2.3 Formula de Clasificare

Pentru a evita **underflow-ul numeric** (Ã®nmulÈ›irea multor probabilitÄƒÈ›i mici), folosim **logaritmi**:

```
score(C) = log P(C) + Î£ log P(wáµ¢|C)
```

Categoria prezisÄƒ este cea cu scorul maxim:

```
C* = argmax_C [log P(C) + Î£ log P(wáµ¢|C)]
```

### 2.4 Laplace Smoothing

Pentru a evita probabilitÄƒÈ›ile de **zero** (cÃ¢nd un cuvÃ¢nt nu apare Ã®n setul de antrenament pentru o anumitÄƒ categorie), folosim **Laplace Smoothing** (add-one smoothing):

```
P(wáµ¢|C) = (count(wáµ¢, C) + Î±) / (count_total(C) + Î± Ã— |V|)
```

Unde:
- **count(wáµ¢, C)** = numÄƒrul de apariÈ›ii al cuvÃ¢ntului wáµ¢ Ã®n categoria C
- **count_total(C)** = numÄƒrul total de cuvinte Ã®n categoria C
- **Î±** = parametrul de smoothing (Ã®n implementarea noastrÄƒ Î± = 1)
- **|V|** = dimensiunea vocabularului

### 2.5 Stabilitate NumericÄƒ

Folosirea logaritmilor transformÄƒ:
- **ÃnmulÈ›iri** â†’ **AdunÄƒri** (mai rapid computaÈ›ional)
- **ProbabilitÄƒÈ›i mici** â†’ **Valori negative controlabile** (evitÄƒ underflow)

---

## 3. Structura Codului

### 3.1 Arhitectura GeneralÄƒ

Proiectul este organizat Ã®n **3 faze principale**:

```
main.py
â”œâ”€â”€ FAZA 1: Prepararea È™i CurÄƒÈ›area Datelor
â”‚   â”œâ”€â”€ 1.1 ÃncÄƒrcarea datelor (pandas)
â”‚   â”œâ”€â”€ 1.2 Extragerea categoriilor din URL-uri
â”‚   â”œâ”€â”€ 1.3 Simplificarea È™i filtrarea categoriilor
â”‚   â”œâ”€â”€ 1.4 CurÄƒÈ›area È™i tokenizarea textului
â”‚   â””â”€â”€ 1.5 ÃmpÄƒrÈ›irea Ã®n train/test (80/20)
â”‚
â”œâ”€â”€ FAZA 2: Implementarea Clasificatorului
â”‚   â”œâ”€â”€ 2.1 FuncÈ›ia train() - Ã®nvÄƒÈ›are parametri
â”‚   â””â”€â”€ 2.2 FuncÈ›ia predict() - clasificare text nou
â”‚
â””â”€â”€ FAZA 3: Evaluarea Modelului
    â”œâ”€â”€ 3.1 Rularea predicÈ›iilor pe setul de test
    â””â”€â”€ 3.2 Calcularea acurateÈ›ei È™i metrici
```

### 3.2 FuncÈ›ii Principale

#### **3.2.1 `extract_category(url)`**
```python
def extract_category(url):
    """Extrage categoria dintr-un URL BBC News."""
```
- **Input**: URL complet (ex: `https://www.bbc.co.uk/news/business-12345`)
- **Output**: Categoria extrasÄƒ (ex: `business`)
- **MetodÄƒ**: Expresii regulate (regex) pentru pattern matching

#### **3.2.2 `simplify_category(cat)`**
```python
def simplify_category(cat):
    """GrupeazÄƒ categoriile similare Ã®n mega-categorii."""
```
- **Input**: Categorie originalÄƒ (ex: `world-europe`)
- **Output**: Categorie simplificatÄƒ (ex: `world`)
- **LogicÄƒ**: GrupeazÄƒ categoriile semantice similare pentru Ã®mbunÄƒtÄƒÈ›irea acurateÈ›ei

**Mapare categorii:**
| Categorii Originale | Categorie FinalÄƒ |
|---------------------|------------------|
| world-europe, world-asia, world-us-canada, etc. | **world** |
| uk-politics, uk-scotland, uk-wales, etc. | **uk** |
| entertainment-arts, newsbeat | **entertainment** |
| technology, science-environment | **tech-science** |
| health, education | **health-education** |
| business | **business** |

#### **3.2.3 `clean_text(text)`**
```python
def clean_text(text):
    """CurÄƒÈ›Äƒ È™i tokenizeazÄƒ textul."""
```
- **Input**: Text brut (titlu + descriere)
- **Output**: ListÄƒ de tokens (cuvinte procesate)

**PaÈ™i de procesare:**
1. **Lowercase**: Conversie la litere mici (`"Bitcoin" â†’ "bitcoin"`)
2. **Eliminare punctuaÈ›ie**: PÄƒstreazÄƒ doar litere È™i spaÈ›ii
3. **Tokenizare**: Ãmparte textul Ã®n cuvinte individuale
4. **Stop words removal**: EliminÄƒ cuvinte comune (`the`, `a`, `is`, etc.)
5. **Filtrare lungime**: EliminÄƒ cuvinte cu < 3 caractere

#### **3.2.4 `train(train_data)`**
```python
def train(train_data):
    """ÃnvaÈ›Äƒ parametrii modelului Naive Bayes."""
```

**ReturneazÄƒ 3 structuri de date:**

1. **`vocabulary`** (set): Toate cuvintele unice din setul de antrenament
   ```python
   vocabulary = {'bitcoin', 'economy', 'minister', ...}  # 28,197 cuvinte
   ```

2. **`prior_probs`** (dict): ProbabilitÄƒÈ›i a priori P(C)
   ```python
   prior_probs = {
       'business': 0.132,
       'uk': 0.485,
       'world': 0.420,
       ...
   }
   ```

3. **`cond_probs`** (dict nested): ProbabilitÄƒÈ›i condiÈ›ionate P(w|C)
   ```python
   cond_probs = {
       'business': {
           'economy': 0.0023,
           'market': 0.0019,
           ...
       },
       'uk': {...},
       ...
   }
   ```

**Algoritm:**
```
Pentru fiecare categorie C:
    1. CalculeazÄƒ P(C) = count(C) / total_documents
    2. Pentru fiecare cuvÃ¢nt w Ã®n vocabular:
        a. NumÄƒrÄƒ apariÈ›ii: count(w, C)
        b. AplicÄƒ Laplace: P(w|C) = (count + 1) / (total + |V|)
```

#### **3.2.5 `predict(tokens, vocabulary, prior_probs, cond_probs)`**
```python
def predict(tokens, vocabulary, prior_probs, cond_probs):
    """ClasificÄƒ un text pe baza token-ilor sÄƒi."""
```

**Algoritm:**
```
Pentru fiecare categorie C:
    1. score = log P(C)
    2. Pentru fiecare cuvÃ¢nt w Ã®n text:
        a. DacÄƒ w Ã®n vocabular:
            score += log P(w|C)
    3. ReturneazÄƒ C cu score maxim
```

**Exemplu de calcul:**
```
Text: "UK minister announces new policy"
Tokens: ['minister', 'announces', 'policy']

Score(uk) = log(0.485) + log(P('minister'|uk)) + log(P('announces'|uk)) + log(P('policy'|uk))
          = -0.723 + (-5.2) + (-6.1) + (-5.8)
          = -17.823

Score(world) = log(0.420) + ...
             = -19.456

PredicÈ›ie: 'uk' (scor mai mare)
```

### 3.3 Biblioteci Utilizate

```python
import pandas as pd       # Manipulare date CSV
import numpy as np        # Calcule matematice (logaritmi)
import re                 # Expresii regulate (procesare URL)
from collections import defaultdict  # DicÈ›ionare cu valori default
```

**NotÄƒ**: Nu am folosit biblioteci de ML (scikit-learn, NLTK) - totul implementat manual!

---

## 4. InstrucÈ›iuni de Utilizare

### 4.1 CerinÈ›e de Sistem

- **Python**: 3.8 sau superior
- **Sistem Operare**: Windows, macOS, Linux
- **Memorie RAM**: Minimum 2GB (recomandat 4GB)

### 4.2 Instalarea DependenÈ›elor

```bash
# CreeazÄƒ un mediu virtual (opÈ›ional, dar recomandat)
python3 -m venv venv
source venv/bin/activate  # Linux/macOS
# sau
venv\Scripts\activate     # Windows

# InstaleazÄƒ bibliotecile necesare
pip install pandas numpy
```

### 4.3 Structura FiÈ™ierelor

AsigurÄƒ-te cÄƒ ai urmÄƒtoarea structurÄƒ:

```
Proiect_Naive_Bayes/
â”œâ”€â”€ main.py                  # Script principal
â”œâ”€â”€ bbc_news.csv             # Dataset 
â””â”€â”€ documentatie.md          # Acest document (pentru Github)
```

### 4.4 Rularea Programului

```bash
# NavigheazÄƒ Ã®n directorul proiectului
cd Proiect_Naive_Bayes

# RuleazÄƒ scriptul principal
python3 main.py
```

### 4.5 Output AÈ™teptat

Programul va afiÈ™a:
1. âœ… Progresul procesÄƒrii datelor (Faza 1)
2. âœ… Antrenarea modelului (Faza 2)
3. âœ… Rezultatele evaluÄƒrii (Faza 3):
   - AcurateÈ›ea globalÄƒ
   - AcurateÈ›ea per categorie
   - 10 exemple de predicÈ›ii

**Timpul de execuÈ›ie**: ~15-30 secunde (depinde de procesor)

---

## 5. Exemple de Utilizare

### 5.1 Clasificare Articol Nou

Pentru a clasifica un articol nou, foloseÈ™te funcÈ›ia `predict()`:

```python
# Text nou de clasificat
new_title = "Tesla stock surges after quarterly earnings report"
new_description = "Electric vehicle maker announces record profits"

# ProceseazÄƒ textul
text = new_title + ' ' + new_description
tokens = clean_text(text)

# ClasificÄƒ
predicted_category = predict(tokens, vocabulary, prior_probs, cond_probs)

print(f"Categorie prezisÄƒ: {predicted_category}")
# Output: "Categorie prezisÄƒ: business"
```

### 5.2 Exemple de PredicÈ›ii Corecte

**Exemplu 1 - Business:**
```
Titlu: "Long-term sick: How record number changes UK economy"
Tokens: ['long', 'term', 'sick', 'record', 'number', 'changes', 'economy']
PredicÈ›ie: business âœ…
```

**Exemplu 2 - World:**
```
Titlu: "Israel-Gaza war: Unknown fate of six-year-old Hind Rajab"
Tokens: ['israel', 'gaza', 'war', 'unknown', 'fate', 'year', 'old', 'hind', 'rajab']
PredicÈ›ie: world âœ…
```

**Exemplu 3 - Entertainment:**
```
Titlu: "Coronation Street drops out of Christmas TV top 10"
Tokens: ['coronation', 'street', 'drops', 'christmas', 'top']
PredicÈ›ie: entertainment âœ…
```

### 5.3 Analiza GreÈ™elilor

**Exemplu de predicÈ›ie greÈ™itÄƒ:**
```
Titlu: "New AI model achieves breakthrough in medical diagnosis"
Tokens: ['new', 'model', 'achieves', 'breakthrough', 'medical', 'diagnosis']
Categorie realÄƒ: health-education
PredicÈ›ie: tech-science âŒ

Motiv: Vocabularul se suprapune Ã®ntre tech-science È™i health-education
```

---

## 6. Rezultate È™i Evaluare

### 6.1 AcurateÈ›ea GlobalÄƒ

```
ğŸ“Š ACURATEÈšEA MODELULUI: 79.30%
   - PredicÈ›ii corecte: 3,950 / 4,981
   - PredicÈ›ii greÈ™ite: 1,031 / 4,981
```

### 6.2 AcurateÈ›e per Categorie

| Categorie | AcurateÈ›e | PredicÈ›ii Corecte/Total |
|-----------|-----------|-------------------------|
| **world** | 86.38% | 1,440 / 1,667 |
| **uk** | 82.65% | 1,620 / 1,960 |
| **business** | 78.36% | 402 / 513 |
| **entertainment** | 73.85% | 322 / 436 |
| **health-education** | 47.34% | 98 / 207 |
| **tech-science** | 34.34% | 68 / 198 |

### 6.3 Interpretarea Rezultatelor

**Categorii cu performanÈ›Äƒ excelentÄƒ (>80%):**
- `world` È™i `uk` au vocabular foarte distinct (nume de locuri, figuri politice locale)
- BeneficiazÄƒ de cel mai mare numÄƒr de exemple de antrenament

**Categorii cu performanÈ›Äƒ medie (70-80%):**
- `business` È™i `entertainment` au vocabular mai specific

**Categorii cu performanÈ›Äƒ mai slabÄƒ (<50%):**
- `health-education` È™i `tech-science` au cel mai puÈ›in date de antrenament
- Vocabularul se suprapune cu alte categorii (ex: "study", "research")

### 6.4 ComparaÈ›ie cu Baseline

| ConfiguraÈ›ie | Nr. Categorii | AcurateÈ›e |
|--------------|---------------|-----------|
| Random Guess | 6 | 16.67% |
| Always Predict "uk" | 6 | 39.35% |
| **Naive Bayes (implementat)** | **6** | **79.30%** |

Modelul nostru depÄƒÈ™eÈ™te cu mult ambele baseline-uri!

### 6.5 AnalizÄƒ ExperimentalÄƒ

Am experimentat cu **3 configuraÈ›ii** de categorii:

| ConfiguraÈ›ie | Nr. Categorii | AcurateÈ›e | ÃmbunÄƒtÄƒÈ›ire |
|--------------|---------------|-----------|--------------|
| OriginalÄƒ (granularÄƒ) | 25 | 54.19% | - |
| ConservativÄƒ | ~15 | ~62% | +7.81% |
| **AgresivÄƒ (finalÄƒ)** | **6** | **79.30%** | **+25.11%** |

**Concluzie**: Gruparea categoriilor similare Ã®mbunÄƒtÄƒÈ›eÈ™te semnificativ acurateÈ›ea!

---

## 7. ReferinÈ›e Bibliografice

1. **Materialele Cursului** - *ProbabilitaÈ›i È™i statisticÄƒ*  
   Universitatea din BucureÈ™ti, 2025 
   - Laboratoare È™i suport de curs pentru algoritmul Naive Bayes

2. **Dataset** - Preda, G. (2020). *BBC News Dataset*. Kaggle.  
   - https://www.kaggle.com/datasets/gpreda/bbc-news/data  
   - Dataset cu 42,115 articole BBC (2013-2024)

3. **Wikipedia** - *Naive Bayes classifier*  
   - https://en.wikipedia.org/wiki/Naive_Bayes_classifier  
   - ReferinÈ›Äƒ pentru formula matematicÄƒ È™i Laplace Smoothing

4. **StatQuest with Josh Starmer** - *Naive Bayes, Clearly Explained!!!*. YouTube.  
   - https://www.youtube.com/watch?v=O2L2Uv9pdDA  
   - ExplicaÈ›ie vizualÄƒ a algoritmului

5. **Python Documentation**  
   - pandas: https://pandas.pydata.org/docs/  
   - numpy: https://numpy.org/doc/  
   - re (regular expressions): https://docs.python.org/3/library/re.html

---

## AnexÄƒ: ObservaÈ›ii

### Puncte Forte ale ImplementÄƒrii
âœ… Cod clar, bine documentat È™i modular  
âœ… Implementare completÄƒ de la zero (fÄƒrÄƒ scikit-learn)  
âœ… Optimizare categorii pentru acurateÈ›e maximÄƒ  
âœ… Laplace Smoothing implementat corect  
âœ… Folosirea logaritmilor pentru stabilitate numericÄƒ  

---

**Data finalizÄƒrii documentaÈ›iei:** 16 Noiembrie 2025  
**Versiune:** 1.0